{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "L1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NehaKoppikar/BTDSS6DL/blob/master/Theory/L1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8yaC7rxowlT",
        "colab_type": "text"
      },
      "source": [
        "RECAP: WHY DO WE NEED ML?\n",
        "\n",
        "1. When we don't know how to program something because you don't know how it works\n",
        "2. When there might be a large number of rules that change easily\n",
        "3. But when we have a large number of cases where we know the outputs\n",
        "4. Recognizing patterns - recognizing absence of patterns (i.e. anamolies)\n",
        "5. Predicting the future"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgztGwB7owlX",
        "colab_type": "text"
      },
      "source": [
        "Visual Pattern recognition - Textbook case for applying ML\n",
        "- Visual patterns intuitive to recognize \n",
        "- Extremely hard to define\n",
        "- Huge collection of internet users generate lots of labelled data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P--ILKOmowlZ",
        "colab_type": "text"
      },
      "source": [
        "Magic of ML\n",
        "- Relatively few lines of code + Tons of Data + Tons of Compute = Solution to this hard problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9t0rfszowlf",
        "colab_type": "text"
      },
      "source": [
        "Think of yourself as an online clothes marketplace.\n",
        "Hundreds of vendors joining your website\n",
        "Millions of customers\n",
        "You need some automatic way of recommending the products to the right customers\n",
        "Neither the vendor or the customer are going to fill lengthy forms!\n",
        "You need to automate this process\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DKVe9kRowli",
        "colab_type": "text"
      },
      "source": [
        "Fashion-MNIST is a dataset of Zalando's article images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv1lJ-T2owll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load the dataset from Keras\n",
        "from keras.datasets import fashion_mnist\n",
        "((trainX, trainY), (testX, testY)) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E3yyIUJowl2",
        "colab_type": "text"
      },
      "source": [
        "Consists of \n",
        "a training set of 60,000 examples \n",
        "a test set of 10,000 examples. \n",
        "\n",
        "Each example is a 28x28 grayscale image, associated with a label from 10 classes. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZvGHzngowl_",
        "colab_type": "code",
        "colab": {},
        "outputId": "8b311239-82e3-4bc0-e077-03231f667598"
      },
      "source": [
        "#Check the number of training and test examples\n",
        "print(\"Training Samples:\", len(trainX))\n",
        "print(\"Test Samples:\", len(testX))\n",
        "\n",
        "#Assign Labels\n",
        "labels=dict( {\n",
        "\"0\": \"T-shirt/top\",\n",
        "\"1\": \"Trouser\",\n",
        "\"2\": \"Pullover\",\n",
        "\"3\": \"Dress\",\n",
        "\"4\": \"Coat\",\n",
        "\"5\": \"Sandal\",\n",
        "\"6\": \"Shirt\",\n",
        "\"7\": \"Sneaker\",\n",
        "\"8\": \"Bag\",\n",
        "\"9\": \"Ankle boot\"\n",
        "})\n",
        "\n",
        "#Takes in an index and print the corresponding label\n",
        "def label(index):\n",
        "    return labels[str(index)]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Samples: 60000\n",
            "Test Samples: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3mfBCdtowmU",
        "colab_type": "text"
      },
      "source": [
        "Visualize a scaled up version of the data to gain intuition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5ZYRjalowmV",
        "colab_type": "code",
        "colab": {},
        "outputId": "9bd37688-69d0-4068-fd7e-5cb529fb6609"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import pylab as pl\n",
        "from IPython import display\n",
        "\n",
        "num_images_to_viz=10\n",
        "for i in range(num_images_to_viz):\n",
        "\n",
        "    plt.imshow(trainX[i],cmap='gray')\n",
        "    plt.title(label(trainY[i]))\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(pl.gcf())\n",
        "    time.sleep(3.0)    \n",
        "\n",
        "print(trainX[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0\n",
            "    0   1   4   0   0   0   0   1   1   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62\n",
            "   54   0   0   0   1   3   4   0   0   3]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134\n",
            "  144 123  23   0   0   0   0  12  10   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178\n",
            "  107 156 161 109  64  23  77 130  72  15]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216\n",
            "  216 163 127 121 122 146 141  88 172  66]\n",
            " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229\n",
            "  223 223 215 213 164 127 123 196 229   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228\n",
            "  235 227 224 222 224 221 223 245 173   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198\n",
            "  180 212 210 211 213 223 220 243 202   0]\n",
            " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192\n",
            "  169 227 208 218 224 212 226 197 209  52]\n",
            " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203\n",
            "  198 221 215 213 222 220 245 119 167  56]\n",
            " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240\n",
            "  232 213 218 223 234 217 217 209  92   0]\n",
            " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219\n",
            "  222 221 216 223 229 215 218 255  77   0]\n",
            " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208\n",
            "  211 218 224 223 219 215 224 244 159   0]\n",
            " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230\n",
            "  224 234 176 188 250 248 233 238 215   0]\n",
            " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223\n",
            "  255 255 221 234 221 211 220 232 246   0]\n",
            " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221\n",
            "  188 154 191 210 204 209 222 228 225   0]\n",
            " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117\n",
            "  168 219 221 215 217 223 223 224 229  29]\n",
            " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245\n",
            "  239 223 218 212 209 222 220 221 230  67]\n",
            " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216\n",
            "  199 206 186 181 177 172 181 205 206 115]\n",
            " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191\n",
            "  195 191 198 192 176 156 167 177 210  92]\n",
            " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209\n",
            "  210 210 211 188 188 194 192 216 170   0]\n",
            " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179\n",
            "  182 182 181 176 166 168  99  58   0   0]\n",
            " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqmKGrmYowmd",
        "colab_type": "text"
      },
      "source": [
        "Visualize the data at the original scale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFoFQUiXowmj",
        "colab_type": "code",
        "colab": {},
        "outputId": "491f7b20-7087-494b-b5b8-c5b7feadb18b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "dpi = 28\n",
        "margin = 0.05 # (5% of the width/height of the figure...)\n",
        "xpixels, ypixels = 28, 28\n",
        "\n",
        "# Make a figure big enough to accomodate an axis of xpixels by ypixels\n",
        "# as well as the ticklabels, etc...\n",
        "figsize = (1 + margin) * ypixels / dpi, (1 + margin) * xpixels / dpi\n",
        "\n",
        "for i in range(num_images_to_viz):\n",
        "    fig = plt.figure(figsize=figsize, dpi=dpi)\n",
        "    print(label(trainY[i]))   \n",
        "            \n",
        "    # Make the axis the right size...\n",
        "    ax = fig.add_axes([margin, margin, 1 - 2*margin, 1 - 2*margin])\n",
        "\n",
        "    ax.imshow(trainX[i],cmap='gray', interpolation='none')\n",
        "    #fig.suptitle(label(trainY[i]),fontsize=16,pad=10)\n",
        "    plt.show()\n",
        "\n",
        "    display.display(pl.gcf())\n",
        "    time.sleep(3.0)\n",
        "    display.clear_output(wait=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWFJL1llowms",
        "colab_type": "text"
      },
      "source": [
        "# Perceptrons : Intuitive Way in which we make decisions\n",
        "\n",
        "For every yes / no decision\n",
        "We have certain factors that we consider\n",
        "We assign a certain importance to those factors\n",
        "Finally if the weighted importance is greater than some imaginary threshold, we go ahead with the decision\n",
        "\n",
        "Life comes down to \n",
        "1. Identifying the important factors [unsupervised learning]\n",
        "2. Learning how to weigh them [supervised learning]\n",
        "3. Fine-tuning an intuitive threshold [supervised learning]\n",
        "\n",
        "Eg: Should I take up the job offer?\n",
        "Factors: \n",
        "    1. Does it pay well?\n",
        "    2. Do I have to commute long?\n",
        "    3. Do I have to work long hours?\n",
        "    4. Char log kuch kahenge?\n",
        "\n",
        "Weights:\n",
        "    1. Pay: 10\n",
        "    2. Commute: -12\n",
        "    3. Long Hours: -2\n",
        "    4. Char log: 5\n",
        "\n",
        "Threshold: 2\n",
        "\n",
        "Decision: Yes / No"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enB1rsDDowmx",
        "colab_type": "text"
      },
      "source": [
        "In the context of perceptrons, \n",
        "1. one can ask only yes / no questions [i.e. features]\n",
        "2. one can get only yes / no answers [i.e. decisions]\n",
        "3. Weights and biases can be any real number\n",
        "\n",
        "Although simplistic - a very powerful model\n",
        "\n",
        "One can create universal NAND gate. \n",
        "\n",
        "Essentially express *anything* that can be computed as a network of perceptrons.\n",
        "\n",
        "Note: The ability to express a computable function is not the same as the ability to learn it from data! *Nowhere close*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8rnBHcEowmz",
        "colab_type": "text"
      },
      "source": [
        "# Learning from mistakes #\n",
        "\n",
        " - The most reliable way in which humans learn\n",
        " - This is the same way in which we have devised machine learning algorithms to learn\n",
        " - Essentially take lots of training examples and make a lots of mistakes\n",
        " - With each mistake, take a small step (learning rate) towards the correct solution\n",
        " - Can you now see, why these algorithms require lots of compute?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF785bVWowm1",
        "colab_type": "text"
      },
      "source": [
        "# Problem with Perceptrons #\n",
        "\n",
        "1. They take binary input, produce binary ouput.\n",
        "Eg: Isn't it more intuitive to ask\n",
        "\n",
        "Should I take up the job offer?\n",
        "Factors: \n",
        "    1. *How much* does it pay well?\n",
        "    2. *How much* do I have to commute?\n",
        "    3. *How many* hours do I have to work>\n",
        "    4. Char log *kya* kahenge?\n",
        "\n",
        "Weights:\n",
        "    1. Pay: 10\n",
        "    2. Commute: -12\n",
        "    3. Work Hours: -2\n",
        "    4. Char log: 5\n",
        "\n",
        "Threshold: 2\n",
        "\n",
        "Decision: Maybe\n",
        "\n",
        "\n",
        "2. A small change in input may cause the ouput to flip from 0 to 1\n",
        "3. We *want* an algorithm that learns slowly - that doesn't jump to conclusions based upon the current training example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWTk0J_Bowm3",
        "colab_type": "text"
      },
      "source": [
        "# Sigmoid Neurons #\n",
        "\n",
        "Sigmoid Function: \n",
        "\\begin{align}\n",
        "\\sigma(x) &= \\frac{1}{1+e^{-x}} \\\\\n",
        "\\end{align}\n",
        "\n",
        "Derivative of the sigmoid function: \n",
        "\\begin{align}\n",
        "\\dfrac{d}{dx} \\sigma(x) &= \\dfrac{d}{dx} \\left[ \\dfrac{1}{1 + e^{-x}} \\right] \\\\\n",
        "&= \\dfrac{d}{dx} \\left( 1 + \\mathrm{e}^{-x} \\right)^{-1} \\\\\n",
        "&= -(1 + e^{-x})^{-2}(-e^{-x}) \\\\\n",
        "&= \\dfrac{e^{-x}}{\\left(1 + e^{-x}\\right)^2} \\\\\n",
        "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{e^{-x}}{1 + e^{-x}}  \\\\\n",
        "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{(1 + e^{-x}) - 1}{1 + e^{-x}}  \\\\\n",
        "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( \\dfrac{1 + e^{-x}}{1 + e^{-x}} - \\dfrac{1}{1 + e^{-x}} \\right) \\\\\n",
        "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( 1 - \\dfrac{1}{1 + e^{-x}} \\right) \\\\\n",
        "&= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
        "\\end{align}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC3wgaruowm7",
        "colab_type": "code",
        "colab": {},
        "outputId": "5dfb2bbe-6966-4ae1-ca58-b6e31f4128a2"
      },
      "source": [
        "# Import matplotlib, numpy and math \n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np \n",
        "import math \n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x)) \n",
        "\n",
        "x = np.linspace(-10, 10, 100) \n",
        "y = sigmoid(x)*(1-sigmoid(x))\n",
        "z = sigmoid(x)\n",
        "\n",
        "  \n",
        "plt.plot(x, z, label=\"sigmoid\") \n",
        "plt.plot(x, y, label=\"derivative\") \n",
        "plt.xlabel(\"x\") \n",
        "plt.ylabel(\"Sigmoid(X)\") \n",
        "plt.legend()\n",
        "\n",
        "#Note that both the sigmoid and its derivate vanish at the extremes\n",
        "#This fact will be extremely important later on when we encounter vanishing gradients\n",
        "#Neural networks stop learning due to this very reason\n",
        "#This is also why other activation functions like selu, relu, leaky relu, etc. have been invented\n",
        "\n",
        "plt.show() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzYhELXtownb",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network Design #\n",
        "\n",
        "We want\n",
        "1. Some way of feeding in the input [28 x 28 pixels]\n",
        "2. Some way of checking the output [class label]\n",
        "3. Some way of quantifying the mistake made by the neural network.\n",
        "4. Some way of steering the network so that it may make fewer mistakes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ4_tVGLownn",
        "colab_type": "text"
      },
      "source": [
        "First we consider just the initialization of weights and biases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_opJQxhPownp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Req 1: First we concentrate on feeding the input\n",
        "\n",
        "# Convert the 2D input to 1D\n",
        "def convert_to_1D(X):\n",
        "    return [ x.ravel()/256.0 for x in X ]\n",
        "    #Is the normalization needed?\n",
        "\n",
        "trainX_1D=convert_to_1D(trainX)\n",
        "testX_1D=convert_to_1D(testX)\n",
        "\n",
        "# Q1. Why is this conversion needed?\n",
        "# Q2. What if I were to find a way one random order. Then shuffle all the examples (both train + test) using this order?\n",
        "# Q3. What if I were to uses a random sequence for each training example?\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPNyq8Q2owns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Req 2: Now we convert each target output from a single valued label to a 1D array\n",
        "\n",
        "#Takes in a 1D array of targets and returns a 2D one-hot encoding\n",
        "def convert_to_one_hot_encoding(target_Y):\n",
        "    Target_Y=np.zeros((len(target_Y),len(labels)),dtype=np.float64)\n",
        "    for i in range(len(target_Y)):\n",
        "        Target_Y[i][target_Y[i]]=1\n",
        "    return Target_Y\n",
        "\n",
        "#Q1. Why is this needed?\n",
        "#Q2. Again, what if I were to randomize here?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aET215ONownx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Req 3: Quantify the mistake made by the neural network\n",
        "\n",
        "#Returns the mean squared error\n",
        "def mse(output_Y,target_Y):\n",
        "    assert len(output_Y)==len(target_Y)\n",
        "    #convert the target Y to 2D matrix\n",
        "    Target_Y=convert_to_one_hot_encoding(target_Y)\n",
        "    diff=np.subtract(Target_Y, output_Y)\n",
        "    mse=np.mean(diff**2,axis=0)\n",
        "    return 0.5*mse\n",
        "\n",
        "#Q1. Why this particular function?\n",
        "#Q2. What if we use just the difference between the output and the target?\n",
        "#Q3. What if we use error cube instead of error square? \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6bneyWrown4",
        "colab_type": "text"
      },
      "source": [
        "# Gradient Descent #\n",
        "Req 4: Some way of steering the network towards the correct ouput\n",
        "\n",
        "$y$ = $x_{1}^2$ + $x_{2}^2$ \n",
        "\n",
        "$y^\\prime$ = $(x_{1}+\\Delta x_{1})^2$ + $(x_{2}+\\Delta x_{2})^2$ \n",
        "\n",
        "$y^\\prime$ = $x_{1}^2$ + 2*$x_{1}$ * $\\Delta x_{1}$   + $\\Delta x_{1}^2$ +    $x_{2}^2$ + 2*$x_{2}$ * $\\Delta x_{2}$   + $\\Delta x_{2}^2$  \n",
        "\n",
        "$y^\\prime$ - $y$ =  2*$x_{1}$ * $\\Delta x_{1}$   + $\\Delta x_{1}^2$ +  2*$x_{2}$ * $\\Delta x_{2}$   + $\\Delta x_{2}^2$\n",
        "\n",
        "Now if $\\Delta x_1$ and $\\Delta x_2$ are small, say 0.0001; $\\Delta x_1^{2}$ and $\\Delta x_2^{2}$ become even smaller 0.00000001. They can be ignored (for now). [We will encounter techniques where we can't ignore this]\n",
        "\n",
        "\n",
        "$y^\\prime$ - $y$  = $ \\Delta y$ =  2*$x_{1}$ * $\\Delta x_{1}$ +  2*$x_{2}$ * $\\Delta x_{2}$  \n",
        "\n",
        "$\\frac {\\partial y}{\\partial x_{1}}$ =  2 * $x_{1}$\n",
        "\n",
        "$\\frac {\\partial y}{\\partial x_{2}}$ =  2 * $x_{2}$\n",
        "\n",
        "$ \\Delta y$ = [ 2*$x_{1}$  2*$x_{2}$] . [ $\\Delta x_{1}$ $\\Delta x_{2}$]\n",
        "\n",
        "If y was the cost function, then we would want y to always decrease \n",
        "Thus we would want $ \\Delta y$  to *always* be negative\n",
        "\n",
        "The question is how do we change $x_{1}$ (i.e. $\\Delta x_{1}$) and $x_{2}$ (i.e. $\\Delta x_{2}$) such that this *always* happens?\n",
        "\n",
        "What is $ \\Delta y$ if $\\Delta x_{1}$ = -2*$x_{1}$ and $\\Delta x_{2}$ = -2*$x_{2}$ ?\n",
        "\n",
        "$ \\Delta y$ = -2*$x_{1}$ * 2* *$x_{1}$ + -2*$x_{2}$ * 2* *$x_{2}$\n",
        "\n",
        "$ \\Delta y$ = -4*$x_{1}^{2}$ -4* *$x_{2}$\n",
        "\n",
        "$ \\Delta y$ = -4*($x_{1}^{2}$ + $x_{2}^{2}$) = -4 * positive term = negative term\n",
        "\n",
        "Thus by changing the variables in the direction of the gradient, we end up always decreasing the cost.\n",
        "\n",
        "Hence the name *Gradient Descent*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNvvLX58own5",
        "colab_type": "code",
        "colab": {},
        "outputId": "7545c26e-7c00-4148-9878-9ac62264a967"
      },
      "source": [
        "def f(x,y):\n",
        "    return x**2 + y**2\n",
        "\n",
        "\n",
        "def df_dx(x,y):\n",
        "    return 2*x \n",
        "\n",
        "\n",
        "def df_dy(x,y):\n",
        "    return 2*y\n",
        "\n",
        "x,y=-100,100\n",
        "delta_x,delta_y=1000,1000\n",
        "\n",
        "epsilon=0.005\n",
        "\n",
        "lr=.1\n",
        "\n",
        "itr_to_convergence=0\n",
        "while(abs(delta_x) > epsilon or abs(delta_y) > epsilon):\n",
        "    delta_x = -1*lr* df_dx(x,y)\n",
        "    delta_y = -1*lr* df_dy(x,y)\n",
        "    x = x - lr* df_dx(x,y)\n",
        "    y = y - lr* df_dy(x,y)\n",
        "    itr_to_convergence += 1\n",
        "    print(x,y,f(x,y))\n",
        "    \n",
        "print(\"Iterations to convergence:\", itr_to_convergence)\n",
        "\n",
        "#Q1. How does one know that one has converged? f(x) stops growing?\n",
        "#Q2. How does one tune the learning rate?\n",
        "#Q3. What happens if the learning rate is too low? \n",
        "#Q4. What happens if the learning rate is too high? \n",
        "#Q4. Is there a relationship between learning rate and epsilon?"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-80.0 80.0 12800.0\n",
            "-64.0 64.0 8192.0\n",
            "-51.2 51.2 5242.880000000001\n",
            "-40.96 40.96 3355.4432\n",
            "-32.768 32.768 2147.483648\n",
            "-26.2144 26.2144 1374.38953472\n",
            "-20.97152 20.97152 879.6093022208001\n",
            "-16.777216000000003 16.777216000000003 562.9499534213122\n",
            "-13.421772800000003 13.421772800000003 360.28797018963985\n",
            "-10.737418240000002 10.737418240000002 230.58430092136948\n",
            "-8.589934592000002 8.589934592000002 147.57395258967648\n",
            "-6.871947673600002 6.871947673600002 94.44732965739296\n",
            "-5.497558138880001 5.497558138880001 60.44629098073149\n",
            "-4.398046511104001 4.398046511104001 38.685626227668145\n",
            "-3.5184372088832006 3.5184372088832006 24.758800785707614\n",
            "-2.8147497671065604 2.8147497671065604 15.845632502852872\n",
            "-2.251799813685248 2.251799813685248 10.141204801825836\n",
            "-1.8014398509481986 1.8014398509481986 6.490371073168536\n",
            "-1.4411518807585588 1.4411518807585588 4.153837486827863\n",
            "-1.152921504606847 1.152921504606847 2.658455991569832\n",
            "-0.9223372036854777 0.9223372036854777 1.7014118346046927\n",
            "-0.7378697629483821 0.7378697629483821 1.0889035741470032\n",
            "-0.5902958103587057 0.5902958103587057 0.6968982874540821\n",
            "-0.47223664828696454 0.47223664828696454 0.4460149039706125\n",
            "-0.37778931862957166 0.37778931862957166 0.28544953854119204\n",
            "-0.3022314549036573 0.3022314549036573 0.18268770466636292\n",
            "-0.24178516392292587 0.24178516392292587 0.11692013098647228\n",
            "-0.1934281311383407 0.1934281311383407 0.07482888383134226\n",
            "-0.15474250491067257 0.15474250491067257 0.04789048565205905\n",
            "-0.12379400392853805 0.12379400392853805 0.03064991081731779\n",
            "-0.09903520314283044 0.09903520314283044 0.019615942923083385\n",
            "-0.07922816251426436 0.07922816251426436 0.012554203470773368\n",
            "-0.06338253001141149 0.06338253001141149 0.008034690221294957\n",
            "-0.05070602400912919 0.05070602400912919 0.005142201741628772\n",
            "-0.04056481920730336 0.04056481920730336 0.0032910091146424146\n",
            "-0.032451855365842684 0.032451855365842684 0.002106245833371145\n",
            "-0.02596148429267415 0.02596148429267415 0.001347997333357533\n",
            "-0.02076918743413932 0.02076918743413932 0.0008627182933488212\n",
            "-0.016615349947311456 0.016615349947311456 0.0005521397077432456\n",
            "Iterations to convergence: 39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwBtRV6xown-",
        "colab_type": "text"
      },
      "source": [
        "Thus if we can somehow successfully update all the weights and the biases of our neural network in the negative direction of the gradient vector, we will end up reducing the misclassification cost.\n",
        "\n",
        "** How do we change the weights and biases of the neural network? **\n",
        "\n",
        "For each change in each weight and each bias, we want to ensure that the total cost of misclassification across *all* the training examples is minimized.\n",
        "\n",
        "Mathematically we write this as,\n",
        "C(w,b) = $\\frac {1}{2n} \\sum_{x} (pred - y)^{2} $\n",
        "\n",
        "If x (i.e. the number of data points) in the training data set is very large, then computing this cost becomes very expensive.\n",
        "\n",
        "Hence we sample a small set of data points, compute the cost for these data points and consider this average to be equal to the total average\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ6XzaEXown_",
        "colab_type": "code",
        "colab": {},
        "outputId": "938739d6-75f6-42e4-9f0e-3fcbb5f3c5a4"
      },
      "source": [
        "import random\n",
        "array = random.sample(range(1,10000),1000)\n",
        "\n",
        "#Exact mean\n",
        "print(\"Exact Mean:\", np.mean(array))\n",
        "for mini_batch_size in 10, 50, 100, 500, 1000:\n",
        "    temp_array = random.sample( array, mini_batch_size)\n",
        "    print(\"Mini Batch Size:\", mini_batch_size,\" Mean:\", np.mean(temp_array), \"Diff from true mean: \", abs(np.mean(array)-np.mean(temp_array)))\n",
        "    \n",
        "print(\"------\") \n",
        "#over 1000 runs\n",
        "for mini_batch_size in 10, 50, 100, 500, 1000:\n",
        "    diff=[]\n",
        "    for trial in range(100):\n",
        "        temp_array = random.sample( array, mini_batch_size)\n",
        "        diff.append( abs(np.mean(array)-np.mean(temp_array)) )\n",
        "    print(\"Mini Batch Size:\", mini_batch_size,\" Avg Diff from true mean: \", np.mean(diff))\n",
        "    \n",
        "\n",
        "#Stochastic Gradient Descent Way\n",
        "#Epoch is one complete pass through the entire dataset\n",
        "#Note that this is different from sampling (without replacement)\n",
        "print(\"------\") \n",
        "num_epochs=100\n",
        "for mini_batch_size in 10, 50, 100, 500, 1000:\n",
        "    diff=[]\n",
        "    for epoch in range(num_epochs):\n",
        "        random.shuffle(array)\n",
        "        for mb in range(0,len(array),mini_batch_size):\n",
        "           mini_batch_mean=np.mean( array[mb:mb+mini_batch_size] )\n",
        "           diff.append( abs(np.mean(array)-mini_batch_mean)) \n",
        "    print(\"Mini Batch Size:\", mini_batch_size,\" Avg Diff from true mean: \", np.mean(diff))\n",
        "\n",
        "#Q1. What is the difference of increasing the number of epochs\n",
        "#Q2. How is epoch related to the mini_batch_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exact Mean: 4993.369\n",
            "Mini Batch Size: 10  Mean: 4062.3 Diff from true mean:  931.0689999999995\n",
            "Mini Batch Size: 50  Mean: 5399.2 Diff from true mean:  405.83100000000013\n",
            "Mini Batch Size: 100  Mean: 5152.34 Diff from true mean:  158.97100000000046\n",
            "Mini Batch Size: 500  Mean: 5032.972 Diff from true mean:  39.603000000000065\n",
            "Mini Batch Size: 1000  Mean: 4993.369 Diff from true mean:  0.0\n",
            "------\n",
            "Mini Batch Size: 10  Avg Diff from true mean:  824.1137200000003\n",
            "Mini Batch Size: 50  Avg Diff from true mean:  318.90022\n",
            "Mini Batch Size: 100  Avg Diff from true mean:  228.36948\n",
            "Mini Batch Size: 500  Avg Diff from true mean:  75.44414000000002\n",
            "Mini Batch Size: 1000  Avg Diff from true mean:  0.0\n",
            "------\n",
            "Mini Batch Size: 10  Avg Diff from true mean:  740.4062074\n",
            "Mini Batch Size: 50  Avg Diff from true mean:  326.930789\n",
            "Mini Batch Size: 100  Avg Diff from true mean:  228.929428\n",
            "Mini Batch Size: 500  Avg Diff from true mean:  71.39303999999998\n",
            "Mini Batch Size: 1000  Avg Diff from true mean:  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f16Lp99dowoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#assumes Y1 is an array of probabilities\n",
        "#assumes Y2 is just the index\n",
        "def compute_classification_accuracy(Y1,Y2):\n",
        "    assert len(Y1) == len(Y2)\n",
        "    match=np.zeros(len(Y1))\n",
        "    for i in range(len(Y1)):\n",
        "        if np.argmax(Y1[i]) == Y2[i]:\n",
        "            match[i]=1\n",
        "    #print(\"Classification Accuracy:\",sum(match),\" / \",len(match))\n",
        "    return sum(match)/len(match)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3xO5oTsowoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import sample\n",
        "\n",
        "class NN(object):\n",
        "    def __init__(self, num_neurons_each_layer):\n",
        "        self.num_layers = len(num_neurons_each_layer)\n",
        "        self.neurons = num_neurons_each_layer\n",
        "        self.biases = []\n",
        "        for i in range(self.num_layers):\n",
        "            #By convention: 0th layer is input layer. It has no bias\n",
        "            if i==0:\n",
        "                self.biases.append([])\n",
        "                continue\n",
        "            else:\n",
        "                #initialize the baises to random normal distribution mean=0 std=1\n",
        "                self.biases.append(np.random.randn( self.neurons[i] ))\n",
        "                \n",
        "        self.weights=[]\n",
        "        for i in range(self.num_layers):\n",
        "            #By convention: 0th layer is input layer. It has no bias\n",
        "            if i==0:\n",
        "                self.weights.append([])\n",
        "                continue\n",
        "            else:\n",
        "                #weights connecting layer i-1 to layer i\n",
        "                #initialize the weights to random normal distribution mean=0 std=1\n",
        "                self.weights.append(np.random.normal(0, 1, (self.neurons[i-1], self.neurons[i])))\n",
        "        \n",
        "    \n",
        "    def feedforward(self, a):\n",
        "        for i in range(1,self.num_layers):\n",
        "            a=np.matmul(a,self.weights[i])+self.biases[i]\n",
        "            a=sigmoid(a)\n",
        "            #print(a)\n",
        "        return a\n",
        "    \n",
        "    def SGD(self, training_data, epochs=1, mini_batch_size=16, lr=3,test_data=None):\n",
        "        #For each epoch\n",
        "        for epoch in range(epochs):\n",
        "            #Randomly shuffle the training data\n",
        "            random.shuffle(training_data)\n",
        "            #Go through the training data one minibatch at a time\n",
        "            for j in range(0,len(training_data),mini_batch_size):\n",
        "                data = training_data[j:j+mini_batch_size]\n",
        "                outY=feedforward(data)\n",
        "            \n",
        "            #For the data find out the average training error\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBLXo59yowoO",
        "colab_type": "code",
        "colab": {},
        "outputId": "d0ae9f39-be4b-4d58-fc98-ccefdcfaf575"
      },
      "source": [
        "#test out the accuracy of random initialization over 1000 trials\n",
        "trials=1\n",
        "acc=np.zeros(trials)\n",
        "for trial in range(trials):\n",
        "    n=NN([784,30,10])\n",
        "    outY=n.feedforward(trainX_1D)\n",
        "    acc[trial]=compute_classification_accuracy(outY,trainY)\n",
        "    print(mse(outY,trainY))\n",
        "print(\"Avg Train Acc:\",np.mean(acc), \"Std Train Acc:\", np.std(acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.02267259 -0.9543439  -0.99779469 ... -0.56590041 -0.97181877\n",
            "   0.01034487]\n",
            " [ 0.98632836 -0.9012304  -0.877951   ... -0.92342693 -0.99642137\n",
            "  -0.98722188]\n",
            " [ 0.96113385 -0.95293868 -0.99864923 ... -0.57867242 -0.98527423\n",
            "  -0.98780494]\n",
            " ...\n",
            " [-0.02753381 -0.97989879 -0.99850325 ... -0.8390182  -0.99174283\n",
            "  -0.99287756]\n",
            " [ 0.99914594 -0.93334709 -0.99431449 ... -0.98588418 -0.98755514\n",
            "  -0.98430501]\n",
            " [-0.00831667 -0.70055646 -0.99932844 ... -0.65552666 -0.98963497\n",
            "  -0.99957051]]\n",
            "[[5.14046273e-04 9.10772278e-01 9.95594244e-01 ... 3.20243279e-01\n",
            "  9.44431715e-01 1.07016343e-04]\n",
            " [9.72843644e-01 8.12216239e-01 7.70797951e-01 ... 8.52717293e-01\n",
            "  9.92855549e-01 9.74607038e-01]\n",
            " [9.23778276e-01 9.08092126e-01 9.97300286e-01 ... 3.34861769e-01\n",
            "  9.70765316e-01 9.75758607e-01]\n",
            " ...\n",
            " [7.58110900e-04 9.60201643e-01 9.97008750e-01 ... 7.03951534e-01\n",
            "  9.83553849e-01 9.85805850e-01]\n",
            " [9.98292616e-01 8.71136791e-01 9.88661299e-01 ... 9.71967621e-01\n",
            "  9.75265154e-01 9.68856359e-01]\n",
            " [6.91670635e-05 4.90779353e-01 9.98657326e-01 ... 4.29715203e-01\n",
            "  9.79377372e-01 9.99141205e-01]]\n",
            "mean [ 0.0569806  -0.75818601 -0.89184549 -0.32174014  0.00255619 -0.06738573\n",
            " -0.37043199 -0.53717612 -0.87394179 -0.87024957]\n",
            "mse [0.10178949 0.68119312 0.88576358 0.253323   0.12320446 0.08792755\n",
            " 0.37424012 0.47324355 0.85591125 0.84773331]\n",
            "[[2.57023136e-04 4.55386139e-01 4.97797122e-01 ... 1.60121639e-01\n",
            "  4.72215858e-01 5.35081715e-05]\n",
            " [4.86421822e-01 4.06108119e-01 3.85398976e-01 ... 4.26358647e-01\n",
            "  4.96427775e-01 4.87303519e-01]\n",
            " [4.61889138e-01 4.54046063e-01 4.98650143e-01 ... 1.67430884e-01\n",
            "  4.85382658e-01 4.87879303e-01]\n",
            " ...\n",
            " [3.79055450e-04 4.80100822e-01 4.98504375e-01 ... 3.51975767e-01\n",
            "  4.91776924e-01 4.92902925e-01]\n",
            " [4.99146308e-01 4.35568395e-01 4.94330650e-01 ... 4.85983811e-01\n",
            "  4.87632577e-01 4.84428179e-01]\n",
            " [3.45835318e-05 2.45389677e-01 4.99328663e-01 ... 2.14857602e-01\n",
            "  4.89688686e-01 4.99570603e-01]]\n",
            "Avg Train Acc: 0.10646666666666667 Std Train Acc: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YGITgvwowoX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5kpKsBNowoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}